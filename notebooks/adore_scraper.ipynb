{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "063ed657-bd07-4e7b-95fd-505ef5c56fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning page 1...\n",
      "Found 50 products on page 1\n",
      "Scanning page 2...\n",
      "Found 50 products on page 2\n",
      "Scanning page 3...\n",
      "Found 50 products on page 3\n",
      "Scanning page 4...\n",
      "Found 50 products on page 4\n",
      "Scanning page 5...\n",
      "Found 50 products on page 5\n",
      "Reached maximum pages limit (5)\n",
      "\n",
      "Total unique product URLs collected: 250\n",
      "Saved 250 URLs to data\\raw\\product_urls.txt\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class AdoreBeautyScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.adorebeauty.com.au\"\n",
    "        self.skincare_url = \"https://www.adorebeauty.com.au/c/skin-care.html\"\n",
    "        # Create a cloudscraper session\n",
    "        self.scraper = cloudscraper.create_scraper(\n",
    "            browser={\n",
    "                'browser': 'chrome',\n",
    "                'platform': 'windows',\n",
    "                'mobile': False\n",
    "            }\n",
    "        )\n",
    "        self.product_urls = set()\n",
    "\n",
    "    def get_product_urls_from_page(self, page_number):\n",
    "        \"\"\"Extract product URLs from a single page\"\"\"\n",
    "        url = f\"{self.skincare_url}?p={page_number}\"\n",
    "        print(f\"Scanning page {page_number}...\")\n",
    "        \n",
    "        try:\n",
    "            # Use cloudscraper instead of requests\n",
    "            response = self.scraper.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find all product containers\n",
    "            product_containers = soup.find_all('div', class_='relative rounded-md border-[1px] border-[#e1dfdf]')\n",
    "            \n",
    "            # Extract URLs from each container\n",
    "            for container in product_containers:\n",
    "                product_link = container.find('a', href=True)\n",
    "                if product_link and '/p/' in product_link['href']:\n",
    "                    full_url = urljoin(self.base_url, product_link['href'])\n",
    "                    self.product_urls.add(full_url)\n",
    "            \n",
    "            print(f\"Found {len(product_containers)} products on page {page_number}\")\n",
    "            return len(product_containers) > 0  # Return True if products were found\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {page_number}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def collect_all_product_urls(self, max_pages=None):\n",
    "        \"\"\"Iterate through all pages and collect product URLs\"\"\"\n",
    "        page_number = 1\n",
    "        \n",
    "        while True:\n",
    "            if max_pages and page_number > max_pages:\n",
    "                print(f\"Reached maximum pages limit ({max_pages})\")\n",
    "                break\n",
    "                \n",
    "            # Add delay between pages\n",
    "            if page_number > 1:\n",
    "                time.sleep(2)\n",
    "            \n",
    "            # Get URLs from current page\n",
    "            found_products = self.get_product_urls_from_page(page_number)\n",
    "            \n",
    "            # If no products found or error occurred, stop\n",
    "            if not found_products:\n",
    "                print(f\"No more products found after page {page_number-1}\")\n",
    "                break\n",
    "                \n",
    "            page_number += 1\n",
    "        \n",
    "        print(f\"\\nTotal unique product URLs collected: {len(self.product_urls)}\")\n",
    "        return list(self.product_urls)\n",
    "\n",
    "    def save_urls_to_file(self, filename=\"product_urls.txt\"):\n",
    "        \"\"\"Save collected URLs to a file\"\"\"\n",
    "        filepath = os.path.join(\"data\", \"raw\", filename)\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            for url in self.product_urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        \n",
    "        print(f\"Saved {len(self.product_urls)} URLs to {filepath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize scraper\n",
    "    scraper = AdoreBeautyScraper()\n",
    "    \n",
    "    # Collect URLs (limit to 5 pages for testing)\n",
    "    scraper.collect_all_product_urls(max_pages=5)\n",
    "    \n",
    "    # Save URLs to file\n",
    "    scraper.save_urls_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8579c873-aac9-4c8d-a1e0-091ed2178d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b79dd3-1c6f-4b1b-b67e-ab6fef306c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_website = ('https://www.adorebeauty.com.au/c/skin-care.html?p=1')\n",
    "# target_website = ('https://nudieglow.com/collections')\n",
    "target_website = 'https://www.adorebeauty.com.au/p/la-roche-posay/la-roche-posay-cicaplast-baume-b5-100ml.html'\n",
    "request_headers = {\n",
    "    'referer': 'https://www.scrapingcourse.com/ecommerce/',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'content-type': 'application/json',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'sec-ch-device-memory': '8',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n",
    "    'sec-ch-ua-platform': \"Windows\",\n",
    "    'sec-ch-ua-platform-version': '\"10.0.0\"',\n",
    "    'sec-ch-viewport-width': '792',\n",
    "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "  }\n",
    " \n",
    "response = requests.get(target_website, headers=request_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2767a04a-7ca6-42e0-a8f2-578d36fcfa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\"><head><meta charset=\"utf-8\"/><meta content=\"width=device-width,initial-scale=1\" name=\"viewport\"/><meta content=\"px-captcha\" name=\"description\"/><title>Access to this page has been denied</title></head><body><script>window._pxVid='';window._pxUuid='cec88c30-0ef2-11f0-a634-b8ea80eb2ea4';window._pxAppId='PX1Dk3430L';window._pxHostUrl='/1Dk3430L/xhr';window._pxCustomLogo='';window._pxJsClientSrc='/1Dk3430L/init.js';window._pxMobile=false;window._pxFirstPartyEnabled=true;var pxCaptchaSrc='/1Dk3430L/captcha/PX1Dk3430L/captcha.js?a=c&u=cec88c30-0ef2-11f0-a634-b8ea80eb2ea4&v=&m=0&b=aHR0cDovL3d3dy5hZG9yZWJlYXV0eS5jb20uYXUvcC9sYS1yb2NoZS1wb3NheS9sYS1yb2NoZS1wb3NheS10b2xlcmlhbmUtZGVybW8tY2xlYXNuZXIuaHRtbA==';var script=document.createElement('script');script.src=pxCaptchaSrc;script.onload=onScriptLoad;script.onerror=onScriptError;var onScriptErrorCalled;document.head.appendChild(script);var timeoutID=setTimeout(onScriptError,5000);function onScriptLoad(){clearTimeout(timeoutID);setTimeout(function(){if (isCaptchaNotLoaded()){onScriptError();}},1000);}function onScriptError(){if(onScriptErrorCalled){return;}onScriptErrorCalled=true;script=document.createElement('script');script.src='https://captcha.px-cloud.net/PX1Dk3430L/captcha.js?a=c&u=cec88c30-0ef2-11f0-a634-b8ea80eb2ea4&v=&m=0&b=aHR0cDovL3d3dy5hZG9yZWJlYXV0eS5jb20uYXUvcC9sYS1yb2NoZS1wb3NheS9sYS1yb2NoZS1wb3NheS10b2xlcmlhbmUtZGVybW8tY2xlYXNuZXIuaHRtbA==';script.onload=function(){clearTimeout(timeoutID);};script.onerror=window._pxOnError;document.head.appendChild(script);timeoutID=setTimeout(function(){if (isCaptchaNotLoaded()){window._pxOnError();}},5000);}function isCaptchaNotLoaded(){return !document.querySelector('div');}window._pxOnError=function(){var style=document.createElement('style');style.innerText='@import url(https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap);body{background-color:#fafbfc}.px-captcha-error-container{position:fixed;height:340px;background-color:#fff;font-family:Roboto,sans-serif}.px-captcha-error-header{color:#f0f1f2;font-size:29px;margin:67px 0 33px;font-weight:500;line-height:.83;text-align:center}.px-captcha-error-message{color:#f0f1f2;font-size:18px;margin:0 0 29px;line-height:1.33;text-align:center}.px-captcha-error-button{text-align:center;line-height:48px;width:253px;margin:auto;border-radius:50px;border:solid 1px #f0f1f2;font-size:20px;color:#f0f1f2}.px-captcha-error-wrapper{margin:18px 0 0}div.px-captcha-error{margin:auto;text-align:center;width:400px;height:30px;font-size:12px;background-color:#fcf0f2;color:#ce0e2d}img.px-captcha-error{margin:6px 8px -2px 0}.px-captcha-error-refid{border-top:solid 1px #f0eeee;height:27px;margin:13px 0 0;border-radius:0 0 3px 3px;background-color:#fafbfc;font-size:10px;line-height:2.5;text-align:center;color:#b1b5b8}@media (min-width:620px){.px-captcha-error-container{width:530px;top:50%;left:50%;margin-top:-170px;margin-left:-265px;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (min-width:481px) and (max-width:620px){.px-captcha-error-container{width:85%;top:50%;left:50%;margin-top:-170px;margin-left:-42.5%;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (max-width:480px){body{background-color:#fff}.px-captcha-error-header{color:#f0f1f2;font-size:29px;margin:55px 0 33px}.px-captcha-error-container{width:530px;top:50%;left:50%;margin-top:-170px;margin-left:-265px}.px-captcha-error-refid{position:fixed;width:100%;left:0;bottom:0;border-radius:0;font-size:14px;line-height:2}}@media (max-width:390px){div.px-captcha-error{font-size:10px}.px-captcha-error-refid{font-size:11px;line-height:2.5}}';document.head.appendChild(style);var div=document.createElement('div');div.className='px-captcha-error-container';div.innerHTML='<div class=\"px-captcha-error-header\">Before we continue...</div><div class=\"px-captcha-error-message\">Press & Hold to confirm you are<br>a human (and not a bot).</div><div class=\"px-captcha-error-button\">Press & Hold</div><div class=\"px-captcha-error-wrapper\"><div class=\"px-captcha-error\"><img class=\"px-captcha-error\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAAQCAMAAADDGrRQAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAABFUExURUdwTNYELOEGONQILd0AONwALtwEL+AAL9MFLfkJSNQGLdMJLdQJLdQGLdQKLtYFLNcELdUGLdcBL9gFL88OLdUFLNEOLglBhT4AAAAXdFJOUwC8CqgNIRgRoAS1dWWuR4RTjzgryZpYblfkcAAAAI9JREFUGNNdj+sWhCAIhAdvqGVa1r7/oy6RZ7eaH3D4ZACBIed9wlOOMtUnSrEmZ6cHa9YAIfsbCkWrdpi/c50Bk2CO9mNLdMAu03wJA3HpEnfpxbyOg6ruyx8JJi6KNstnslp1dbPd9GnqmuYq7mmcv1zjnbQw8cV0xzkqo+fX1zkjUOO7wnrInUTxJiruC3vtBNRoQQn2AAAAAElFTkSuQmCC\">Please check your internet connection'+(window._pxMobile?'':' or disable your ad-blocker')+'.</div></div><div class=\"px-captcha-error-refid\">Reference ID '+window._pxUuid+'</div>';document.body.appendChild(div);if(window._pxMobile){setTimeout(function(){location.href='/px/captcha_close?status=-1';},5000);}};</script><script src=\"https://www.adorebeauty.com.au/nuxtimages/perimeterx/perimeterx-production-v4.js\"></script></body></html>\n"
     ]
    }
   ],
   "source": [
    "target_website = ('https://www.adorebeauty.com.au/c/skin-care.html?p=2')\n",
    "# target_website = ('https://nudieglow.com/collections')\n",
    "target_website = 'https://www.adorebeauty.com.au/p/la-roche-posay/la-roche-posay-toleriane-dermo-cleasner.html'\n",
    "import cloudscraper\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "soup = BeautifulSoup(scraper.get(target_website).text)\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3d3be98-b595-46dd-99bd-405fa071f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c327d1b5-9661-4bd6-8728-dbb20b9b42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook or Python console\n",
    "scraper = AdoreBeautyScraper()\n",
    "urls = scraper.collect_all_product_urls(max_pages=2)  # Start with 2 pages for testing\n",
    "print(f\"\\nFirst few URLs collected:\")\n",
    "for url in list(urls)[:5]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efb66137-0f91-4745-a736-302810dbf980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Product Data:\n",
      "                                                 url        sku  \\\n",
      "0  https://www.adorebeauty.com.au/p/innisfree/inn...  131174706   \n",
      "\n",
      "                                name      brand  \\\n",
      "0  INNISFREE Energy Mask -  Centella  INNISFREE   \n",
      "\n",
      "                                         description size  \\\n",
      "0  Stressed out skin? Sit back, soothe and hydrat...        \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  <div>water, dipropylene glycol, butylene glyco...   \n",
      "\n",
      "                                          image_urls  \\\n",
      "0  [https://www.adorebeauty.com.au/pim_media/000/...   \n",
      "\n",
      "                  scraped_at  \n",
      "0 2025-04-01 22:14:52.277515  \n",
      "\n",
      "Reviews Data:\n",
      "              review_id product_sku  author                     title  \\\n",
      "0  -4898063170814228968   131174706      RH        Fav Innisfree Mask   \n",
      "1   3912133291945534699   131174706      TL      Not hydrating enough   \n",
      "2     54965610252414939   131174706   Cindy   Soft and refreshed skin   \n",
      "3  -1832191239075145020   131174706      Le                   So good   \n",
      "4   7130804810111649588   131174706  Lauren                Great mask   \n",
      "5  -1932708239804550345   131174706     Ems                      Mask   \n",
      "6   8772354106037289436   131174706     Deb      Love a good facemask   \n",
      "7   8541859710246057956   131174706   Kylie          Soothes the skin   \n",
      "8   8667937706031807547   131174706  Lauren                   Calming   \n",
      "9   8235769591272780693   131174706    Jack  Good for the price point   \n",
      "10 -8501198700346790644   131174706  Rooney          Super refreshing   \n",
      "\n",
      "                                                 body  rating  \\\n",
      "0   I did notice some redness taken away with this...     4.0   \n",
      "1   I love a lot of the masks from this brand but ...     2.0   \n",
      "2   The scent of this mask is fresh and soothing, ...     5.0   \n",
      "3   Such a good product made my face appear less r...     4.0   \n",
      "4   Really liked using this mask - skin felt hydra...     5.0   \n",
      "5              This sheet mask is really hydrating !!     4.0   \n",
      "6   I love sheet face masks as they enable you to ...     3.5   \n",
      "7   This mask soothes the skin and adds lots of hy...     5.0   \n",
      "8           Good facemask will continue to repurchase     5.0   \n",
      "9   A very affordable face sheet mask - the serum ...     2.5   \n",
      "10  Minimal ingredients and really soothing and re...     5.0   \n",
      "\n",
      "         date_published  \n",
      "0   2025-02-17T02:16:55  \n",
      "1   2025-02-03T04:12:02  \n",
      "2   2025-01-27T04:47:25  \n",
      "3   2025-01-12T01:59:35  \n",
      "4   2024-12-27T00:11:14  \n",
      "5   2024-11-27T02:45:18  \n",
      "6   2024-08-01T04:51:24  \n",
      "7   2024-07-23T04:23:22  \n",
      "8   2024-07-23T07:01:07  \n",
      "9   2024-07-23T06:30:09  \n",
      "10  2024-07-23T06:26:21  \n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "class AdoreProductScraper:\n",
    "    def __init__(self):\n",
    "        self.scraper = cloudscraper.create_scraper(\n",
    "        )\n",
    "\n",
    "    def extract_reviews(self, product_data):\n",
    "        \"\"\"Extract reviews from product data into a separate DataFrame\"\"\"\n",
    "        reviews = product_data.get('review', [])\n",
    "        reviews_data = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            review_data = {\n",
    "                'review_id': hash(review.get('reviewBody', '')),  # Create unique ID\n",
    "                'product_sku': product_data.get('sku'),\n",
    "                'author': review.get('author', {}).get('name'),\n",
    "                'title': review.get('name'),\n",
    "                'body': review.get('reviewBody'),\n",
    "                'rating': review.get('reviewRating', {}).get('ratingValue'),\n",
    "                'date_published': review.get('datePublished'),\n",
    "            }\n",
    "            reviews_data.append(review_data)\n",
    "            \n",
    "        return reviews_data\n",
    "\n",
    "    def get_product_data(self, url):\n",
    "        \"\"\"Extract product data from the structured JSON data on the page\"\"\"\n",
    "        try:\n",
    "            # Add random delay\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            # Get the page\n",
    "            response = self.scraper.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the script containing product data\n",
    "            script = soup.find('script', {'id': 'product_structured_data'})\n",
    "            if not script:\n",
    "                print(f\"No product data found for {url}\")\n",
    "                return None, None\n",
    "            \n",
    "            # Parse the JSON data\n",
    "            product_data = json.loads(script.string)\n",
    "            \n",
    "            # Extract product details\n",
    "            product_details = {\n",
    "                'url': url,\n",
    "                'sku': product_data.get('sku'),\n",
    "                'name': product_data.get('name'),\n",
    "                'brand': product_data.get('brand', {}).get('name'),\n",
    "                'description': product_data.get('description'),\n",
    "                'size': product_data.get('size'),\n",
    "                'ingredients': product_data.get('material'),\n",
    "                'image_urls': product_data.get('image', []),\n",
    "                'scraped_at': datetime.now()\n",
    "            }\n",
    "            \n",
    "            # Extract reviews\n",
    "            reviews_data = self.extract_reviews(product_data)\n",
    "            \n",
    "            return product_details, reviews_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def scrape_products_from_file(self, urls_file):\n",
    "        \"\"\"Scrape product data for all URLs in a file\"\"\"\n",
    "        # Read URLs from file\n",
    "        with open(urls_file, 'r') as f:\n",
    "            urls = [line.strip() for line in f]\n",
    "        \n",
    "        # Initialize lists for products and reviews\n",
    "        products_data = []\n",
    "        all_reviews = []\n",
    "        \n",
    "        # Scrape each product\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            print(f\"Processing product {i}/{len(urls)}: {url}\")\n",
    "            product_data, reviews_data = self.get_product_data(url)\n",
    "            \n",
    "            if product_data:\n",
    "                products_data.append(product_data)\n",
    "                if reviews_data:\n",
    "                    all_reviews.extend(reviews_data)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        products_df = pd.DataFrame(products_data)\n",
    "        reviews_df = pd.DataFrame(all_reviews)\n",
    "        \n",
    "        return products_df, reviews_df\n",
    "\n",
    "    def save_data(self, products_df, reviews_df, timestamp=None):\n",
    "        \"\"\"Save products and reviews to separate CSV files\"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "        # Save products\n",
    "        products_file = f\"data/raw/products_{timestamp}.csv\"\n",
    "        products_df.to_csv(products_file, index=False)\n",
    "        print(f\"Saved {len(products_df)} products to {products_file}\")\n",
    "        \n",
    "        # Save reviews\n",
    "        reviews_file = f\"data/raw/reviews_{timestamp}.csv\"\n",
    "        reviews_df.to_csv(reviews_file, index=False)\n",
    "        print(f\"Saved {len(reviews_df)} reviews to {reviews_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with a single URL\n",
    "    scraper = AdoreProductScraper()\n",
    "    test_url = \"https://www.adorebeauty.com.au/p/innisfree/innisfree-energy-mask-centella.html\"\n",
    "    \n",
    "    # Get data for single product\n",
    "    product_data, reviews_data = scraper.get_product_data(test_url)\n",
    "    \n",
    "    if product_data:\n",
    "        # Create DataFrames\n",
    "        products_df = pd.DataFrame([product_data])\n",
    "        reviews_df = pd.DataFrame(reviews_data)\n",
    "        \n",
    "        print(\"\\nProduct Data:\")\n",
    "        print(products_df)\n",
    "        print(\"\\nReviews Data:\")\n",
    "        print(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fff109-450a-4fe4-a62e-18d97e343e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
